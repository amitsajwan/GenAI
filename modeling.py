def perform_modeling(
    modeling_script_filename,
    modeling_report_filename,
    x_train_path,
    y_train_path,
    x_val_path,
    y_val_path,
    x_test_path,
    y_test_path,
    model_output_path,
    scaler_path,
    preliminary_analysis_report_content,
    eda_result_summary,
    feature_engineering_report,
    date_index_name
):
    date_index_name_str = str(date_index_name) if date_index_name else 'None'

    modeling_prompt = f"""
**YOU ARE THE AI PROMPT CREATOR FOR PYTHON MODELING SCRIPT GENERATION**

**Your Mission:**
Your primary responsibility is to act as an intelligent decision-maker and prompt engineer. You will receive summaries from upstream data analysis processes (Preliminary Analysis, EDA, Feature Engineering) and various configuration parameters (file paths, names). Based on a thorough analysis of these inputs, you must dynamically construct a highly detailed, explicit, and self-contained "Execution Prompt." This "Execution Prompt" is your *sole output*. It will be passed directly to a non-reasoning `pythonTool`, which will use it to generate a Python script for building, training, evaluating, and saving a stock price prediction model. The Python script generated by the `pythonTool` must prioritize robustness, effective generalization, and address common machine learning challenges like overfitting based on the guidance in the "Execution Prompt" you create.

**Inputs You Have Received (and their values for your reference):**

1.  **`eda_result_summary_input`**: "{eda_result_summary}"
2.  **`feature_engineering_report_input`**: "{feature_engineering_report}"
3.  **`preliminary_analysis_report_input`**: "{preliminary_analysis_report_content}"
4.  **`x_train_path_value`**: "{x_train_path}"
5.  **`y_train_path_value`**: "{y_train_path}"
6.  **`x_val_path_value`**: "{x_val_path}"
7.  **`y_val_path_value`**: "{y_val_path}"
8.  **`x_test_path_value`**: "{x_test_path}"
9.  **`y_test_path_value`**: "{y_test_path}"
10. **`model_output_path_value`**: "{model_output_path}"
11. **`scaler_path_value`**: "{scaler_path}"
12. **`modeling_script_filename_value`**: "{modeling_script_filename}"
13. **`modeling_report_filename_value`**: "{modeling_report_filename}"
14. **`date_index_name_value`**: "{date_index_name_str}"

**Your Decision-Making and Reasoning Process (Follow these steps to determine content for the "Execution Prompt" you will generate):**

**Phase 1: Analyze Input Summaries**
* Scrutinize `eda_result_summary_input` (which is: "{eda_result_summary}"):
    * Outlier Assessment: Identify features with significant outliers. Decide on a specific handling strategy for the `pythonTool`.
    * Skewness Assessment: Identify highly skewed features. Decide on a transformation strategy for the `pythonTool`.
* Scrutinize `feature_engineering_report_input` (which is: "{feature_engineering_report}"):
    * Model Suitability Hints.
    * Feature Importance/Interactions.
* Briefly review `preliminary_analysis_report_input` (which is: "{preliminary_analysis_report_content}") for context.

**Phase 2: Formulate the Modeling Strategy**
* A. Define Specific Preprocessing Augmentations: Formulate concise instructions for the `pythonTool` based on Phase 1 (e.g., "Apply winsorizing to 'column_x' after scaling," "Apply np.log1p to 'column_y' before scaling").
* B. Select the Primary Model: Choose ONE regression model (e.g., RandomForestRegressor). Justify.
* C. Design the Hyperparameter Tuning Plan: Mandate `GridSearchCV` (or `RandomizedSearchCV`), cv>=3. Define the specific Python dictionary string for the search grid (e.g., "{{'n_estimators': [100, 200], 'max_depth': [10, 20]}}"). Scoring: 'neg_root_mean_squared_error' or 'r2'.

**Phase 3: Construct Your Output â€“ The "Execution Prompt" for `pythonTool`**
* Your final output MUST be a single string. This string is the "Execution Prompt".
* This "Execution Prompt" string that you generate MUST be a valid Python f-string (it must start with `f\"\"\"` and end with `\"\"\"`).
* It should be structured to guide the `pythonTool` in creating a Python script with the components described below.
* When generating this f-string, you must use the *actual values* for paths, filenames, etc., that were provided to you in the "Inputs You Have Received" section (e.g., use the value "{modeling_script_filename}" for the script name, not the placeholder name `modeling_script_filename_value`).
* Your reasoned decisions from Phase 2 (model choice, preprocessing steps, hyperparameter grid) must be precisely inserted into the relevant instruction sections.

---
**STRUCTURE FOR EXECUTION PROMPT (Describe the content and layout of the Python f-string you, the Prompt Creator LLM, will generate. Ensure your output string starts with `f\"\"\"` and ends with `\"\"\"`.)**
---

**Your output f-string for the `pythonTool` should achieve the following:**

1.  **Initial Comments:**
    * Start with a Python comment indicating it's a generated script.
    * Include a **Goal** comment: "Generate an executable Python script ('{modeling_script_filename}') to build, train, evaluate, and save a machine learning regression model..." (Here, and in all similar cases below, replace `{modeling_script_filename}` with the actual value you received, e.g., "{modeling_script_filename}").
    * Include an **Overall Objective** comment: "Build and train a machine learning model using data from '{x_train_path}', '{y_train_path}', etc..." (embedding all relevant actual path values).

2.  **Mandatory Rules (as comments or within script logic if appropriate):**
    * Include instructions or ensure the script adheres to: no command-line arguments, no suppressed exceptions, use of `joblib` for model I/O.

3.  **Suggested Python Script Imports:**
    * Instruct the `pythonTool` to include standard imports: `pandas as pd`, `numpy as np`, `joblib`.
    * Instruct import of `GridSearchCV` (or `RandomizedSearchCV` as per your Phase 2C decision) from `sklearn.model_selection`.
    * Instruct import of `mean_absolute_error, mean_squared_error, r2_score` from `sklearn.metrics`.
    * Instruct import of the specific model class you decided on in Phase 2B (e.g., `from sklearn.ensemble import RandomForestRegressor`).
    * Instruct import of necessary `sklearn.preprocessing` modules (e.g., `StandardScaler`, `RobustScaler`) based on your Phase 2A decisions and the primary scaler at `{scaler_path}`.
    * If winsorizing was decided in Phase 2A, suggest importing `from scipy.stats.mstats import winsorize`.

4.  **Python Script Fixed Inputs (as Python variable definitions):**
    * Instruct the `pythonTool` to define script variables for all paths and names. For each corresponding input you received (e.g., `x_train_path_value` which is "{x_train_path}"), create a script variable.
        * Example: `x_train_path_script = '{x_train_path}'`
        * Do this for `y_train_path`, `x_val_path`, `y_val_path`, `x_test_path`, `y_test_path`, `model_output_path`, `scaler_path`, and `date_index_name` (using the value "{date_index_name_str}").

5.  **Python Script Operational Instructions (structured as Python functions):**

    * **Load Data Function:**
        * Instruct `pythonTool` to define a function `load_data(features_path, target_path, index_col_name)`.
        * It should read CSVs for features and target.
        * If `index_col_name` is valid, it sets this as the DataFrame index (parsing as datetime).
        * It ensures the target `y` is returned as a 1D pandas Series.
        * It returns X, y.
        * The script should then call this function for train, validation, and test sets using the path variables defined in step 4.

    * **Preprocess Data Function:**
        * Instruct `pythonTool` to define `preprocess_data(X_train_df, X_val_df, X_test_df, primary_scaler_path)`.
        * It should first create copies of input DataFrames.
        * **[Your reasoned transformation instructions from Phase 2A for operations BEFORE primary scaling go here. Be specific: "Instruct the `pythonTool` to apply `np.log1p` to the column named 'your_decided_column_name' in X_train_df, X_val_df, and X_test_df."]**
        * It then loads the pre-fitted primary scaler from `primary_scaler_path` (which is "{scaler_path}").
        * It applies this scaler to the (potentially transformed) X_train_df, X_val_df, and X_test_df. Instruct to ensure column names and indices are preserved (reconstruct DataFrames if scaler outputs NumPy arrays).
        * **[Your reasoned outlier handling instructions from Phase 2A for operations AFTER primary scaling go here. Be specific: "Instruct the `pythonTool` to apply winsorizing (limits=[0.01, 0.01]) to the scaled column named 'your_decided_column_name' in the scaled X_train, X_val, and X_test DataFrames."]**
        * It returns the fully processed X_train_final, X_val_final, X_test_final.
        * The script should then call this function.

    * **Model Training and Hyperparameter Tuning Function:**
        * Instruct `pythonTool` to define `train_and_tune_model(X_train_processed, y_train_raw)`.
        * Inside, it should instantiate the model you selected in Phase 2B (e.g., `model = RandomForestRegressor(random_state=42)`).
        * It should define a Python dictionary named `param_grid` containing the exact hyperparameter grid string you formulated in Phase 2C (e.g., `param_grid = {{'n_estimators': [100, 200], 'max_depth': [10, 20]}}` - note the double braces tell *you* (the Prompt Creator LLM) to output single braces for the Python dict).
        * It instantiates `GridSearchCV` (or `RandomizedSearchCV`) with the model, `param_grid`, specified cv, scoring, `n_jobs=-1`, `verbose=1`.
        * It ensures `y_train_raw` is a 1D array for fitting.
        * It fits `GridSearchCV`, extracts `best_estimator_` as `final_model`, and prints `best_params_`.
        * It returns `final_model` and `best_params_`.
        * The script should then call this function.

    * **Model Evaluation Function:**
        * Instruct `pythonTool` to define `evaluate_model(model, X_data, y_data, dataset_name)`.
        * It ensures `y_data` is 1D, makes predictions, calculates MAE, MSE, RMSE, RÂ², prints them formatted, and returns a dictionary of these metrics.
        * The script should call this for `final_model` on training, validation, and test sets.

    * **Save Model Function:**
        * Instruct `pythonTool` to define `save_model_artifact(model, path)`.
        * It saves the `model` to `path` using `joblib.dump()` and prints a save message.
        * The script should call this to save `final_model` to the path specified by `{model_output_path}`.

    * **Prepare and Save Evaluation Report Function:**
        * Instruct `pythonTool` to define `generate_report(report_path, chosen_model_name_str, best_params_dict, train_metrics_dict, val_metrics_dict, test_metrics_dict)`.
        * It writes to `report_path` (specified by `{modeling_report_filename}`): the `chosen_model_name_str` (from your Phase 2B decision), `str(best_params_dict)`, and formatted metrics from the input dictionaries.
        * It should also include placeholder text in the report for "Observations and Insights" regarding overfitting, robustness, and next steps.
        * It prints a save message.
        * The script should then call this function.

    * **Main Script Execution Flow:**
        * Instruct the `pythonTool` to wrap all function calls (Load data, Preprocess, Train/Tune, Evaluate, Save Model, Generate Report) within an `if __name__ == '__main__':` block.
        * It should print a final success message upon completion.

"""
    return modeling_prompt
