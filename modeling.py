def perform_modeling(
    modeling_script_filename,
    modeling_report_filename,
    x_train_path,
    y_train_path,
    x_val_path,
    y_val_path,
    x_test_path,
    y_test_path,
    model_output_path,
    scaler_path,
    preliminary_analysis_report_content,
    eda_result_summary,
    feature_engineering_report,
    date_index_name
):
    # Convert date_index_name to 'None' as a string if it's None or empty, for cleaner embedding
    # and for the LLM to understand it might not be applicable.
    date_index_name_str = str(date_index_name) if date_index_name else 'None'

    modeling_prompt = f"""
**YOU ARE THE AI PROMPT CREATOR FOR PYTHON MODELING SCRIPT GENERATION**

**Your Mission:**
Your primary responsibility is to act as an intelligent decision-maker and prompt engineer. You will receive summaries from upstream data analysis processes (Preliminary Analysis, EDA, Feature Engineering) and various configuration parameters (file paths, names). Based on a thorough analysis of these inputs, you must dynamically construct a highly detailed, explicit, and self-contained "Execution Prompt." This "Execution Prompt" is your *sole output*. It will be passed directly to a non-reasoning `pythonTool`, which will use it to generate a Python script for building, training, evaluating, and saving a stock price prediction model. The Python script generated by the `pythonTool` must prioritize robustness, effective generalization, and address common machine learning challenges like overfitting based on the guidance in the "Execution Prompt" you create.

**Inputs You Have Received (and their values for your reference):**

1.  **`eda_result_summary_input`**: "{eda_result_summary}"
2.  **`feature_engineering_report_input`**: "{feature_engineering_report}"
3.  **`preliminary_analysis_report_input`**: "{preliminary_analysis_report_content}"
4.  **`x_train_path_value`**: "{x_train_path}"
5.  **`y_train_path_value`**: "{y_train_path}"
6.  **`x_val_path_value`**: "{x_val_path}"
7.  **`y_val_path_value`**: "{y_val_path}"
8.  **`x_test_path_value`**: "{x_test_path}"
9.  **`y_test_path_value`**: "{y_test_path}"
10. **`model_output_path_value`**: "{model_output_path}"
11. **`scaler_path_value`**: "{scaler_path}"
12. **`modeling_script_filename_value`**: "{modeling_script_filename}"
13. **`modeling_report_filename_value`**: "{modeling_report_filename}"
14. **`date_index_name_value`**: "{date_index_name_str}"

**Your Decision-Making and Reasoning Process (Follow these steps to determine content for the "Execution Prompt" you will generate):**

**Phase 1: Analyze Input Summaries**
* Scrutinize `eda_result_summary_input` (which is: "{eda_result_summary}"):
    * Outlier Assessment: Identify features with significant outliers. Decide on a specific handling strategy for the `pythonTool`.
    * Skewness Assessment: Identify highly skewed features. Decide on a transformation strategy for the `pythonTool`.
* Scrutinize `feature_engineering_report_input` (which is: "{feature_engineering_report}"):
    * Model Suitability Hints.
    * Feature Importance/Interactions.
* Briefly review `preliminary_analysis_report_input` (which is: "{preliminary_analysis_report_content}") for context.

**Phase 2: Formulate the Modeling Strategy**
* A. Define Specific Preprocessing Augmentations: Formulate concise instructions for the `pythonTool`.
* B. Select the Primary Model: Choose ONE regression model. Justify.
* C. Design the Hyperparameter Tuning Plan: Mandate `GridSearchCV` (or `RandomizedSearchCV`), cv>=3. Define a Python dictionary string for the search grid. Scoring: 'neg_root_mean_squared_error' or 'r2'.

**Phase 3: Construct Your Output â€“ The "Execution Prompt" for `pythonTool`**
* Your final output MUST be a single string. This string is the "Execution Prompt".
* This "Execution Prompt" string that you generate MUST be a valid Python f-string (it must start with `f\"\"\"` and end with `\"\"\"`).
* Use the **"STRUCTURE FOR EXECUTION PROMPT"** described below as a guide. Populate the instructional placeholders (e.g., `[Your reasoned model import...]`) with your decisions.
* When constructing your output f-string, you must use the *actual values* for paths and filenames that were provided to you in the "Inputs You Have Received" section. For example, your output string should contain `'{modeling_script_filename}'` where `modeling_script_filename` is the actual filename like "{modeling_script_filename}", not a placeholder.

---
**STRUCTURE FOR EXECUTION PROMPT (This describes the f-string you, the Prompt Creator LLM, will construct and output. Ensure your output string starts with `f\"\"\"` and ends with `\"\"\"` and correctly embeds the actual values for paths, filenames etc. as shown in the examples below.)**
---

**Your output f-string should contain the following structure, embedding the specific values you received as input (e.g., "{modeling_script_filename}", "{x_train_path}") and your reasoned decisions:**

`f\"\"\"`
`# Start of the Execution Prompt string generated by the Prompt Creator LLM.`

`**Goal:** Generate an executable Python script ('{modeling_script_filename}') to build, train, evaluate, and save a machine learning regression model for stock price prediction. The script must follow all instructions precisely.`

`**Overall Objective:** Build and train a machine learning model using data from '{x_train_path}', '{y_train_path}', '{x_val_path}', '{y_val_path}', '{x_test_path}', and '{y_test_path}'.`

`**Mandatory Rules for Python code generation:**`
`# (List the rules: no script arguments, do not suppress exceptions, use joblib for model I/O)`

`**Suggested Python Script Imports (for the '{modeling_script_filename}'):**`
`import pandas as pd`
`import numpy as np`
`import joblib`
`from sklearn.model_selection import GridSearchCV # Or RandomizedSearchCV, as you decided in Phase 2C`
`from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score`
`# [Prompt Creator: Insert model import based on your Phase 2B decision. E.g., from sklearn.ensemble import RandomForestRegressor]`
`# [Prompt Creator: Insert relevant scaler imports based on your Phase 2A decision and the primary scaler. E.g., from sklearn.preprocessing import StandardScaler, RobustScaler]`
`# [Prompt Creator: If winsorizing decided as part of Phase 2A strategy: from scipy.stats.mstats import winsorize]`

`**Python Script Fixed Inputs (Define these at the top of '{modeling_script_filename}'):**`
`x_train_path_script = '{x_train_path}'`
`y_train_path_script = '{y_train_path}'`
`x_val_path_script = '{x_val_path}'`
`y_val_path_script = '{y_val_path}'`
`x_test_path_script = '{x_test_path}'`
`y_test_path_script = '{y_test_path}'`
`model_output_path_script = '{model_output_path}'`
`scaler_path_script = '{scaler_path}'`
`date_index_name_script = '{date_index_name_str}' # This will be 'None' if date_index_name was None/empty`

`**Python Script Operational Instructions (for '{modeling_script_filename}'):**`

`    # Subsection: 1. Load Data Function`
`    # (Define function load_data(features_path, target_path, index_col_name): reads CSVs, sets index if valid (parse as datetime), ensures y is 1D pandas Series. Returns X, y. Call for train, val, test.)`

`    # Subsection: 2. Preprocess Data Function`
`    # (Define function preprocess_data(X_train_df, X_val_df, X_test_df, primary_scaler_path): Copy DataFrames. [Your reasoned transformation instructions from Phase 2A for operations BEFORE primary scaling, specifying columns, e.g., "Apply np.log1p to the 'column_name' in X_train_df..."]. Load primary scaler from '{scaler_path}'. Apply scaler. [Your reasoned outlier handling instructions from Phase 2A for operations AFTER primary scaling, specifying columns, e.g., "Apply winsorizing to the 'column_name' in X_train_scaled..."]. Returns X_train_processed, X_val_processed, X_test_processed. Call function.)`

`    # Subsection: 3. Model Training and Hyperparameter Tuning Function`
`    # (Define function train_and_tune_model(X_train_processed, y_train_raw): [Your chosen model instantiation from Phase 2B, e.g., model = RandomForestRegressor(random_state=42)]. [Your chosen hyperparameter grid as a Python dictionary string from Phase 2C, e.g., param_grid = {{ 'n_estimators': [50,100], 'max_features': ['sqrt'] }}]. Instantiate GridSearchCV (cv, scoring, n_jobs, verbose=1). Ensure y_train_raw is 1D for fit. Fit GridSearchCV. Extract best_estimator_ as final_model. Print best_params_. Returns final_model, best_params_. Call function.)`

`    # Subsection: 4. Model Evaluation Function`
`    # (Define function evaluate_model(model, X_data, y_data, dataset_name): Ensure y_data is 1D. Predict. Calculate MAE, MSE, RMSE, R2. Print metrics. Returns metrics dict. Call for train, val, test sets.)`

`    # Subsection: 5. Save Model Function`
`    # (Define function save_model_artifact(model, path): joblib.dump(model, path). Print save message. Call to save final_model to '{model_output_path}'.)`

`    # Subsection: 6. Prepare and Save Evaluation Report Function`
`    # (Define function generate_report(report_path, chosen_model_name_str, best_params_dict, train_metrics_dict, val_metrics_dict, test_metrics_dict): Write to '{modeling_report_filename}': chosen_model_name_str [from Phase 2B], str(best_params_dict), formatted metrics. Include placeholder text for Observations/Insights. Print save message. Call function.)`

`    # Subsection: 7. Main Script Execution Flow`
`    # (Wrap calls in if __name__ == '__main__': block: Load data, Preprocess, Train/Tune, Evaluate, Save Model, Generate Report. Print final success message.)`
`\"\"\"`
`# End of the Execution Prompt string to be generated by the Prompt Creator LLM.`

"""
    return modeling_prompt
