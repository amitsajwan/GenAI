**Goal:**
Generate a Python script named `feature.py` to perform feature engineering on the provided cleaned datasets. The script should handle missing values, perform feature scaling, and encode categorical features consistently across train, validation, and test sets. It must also separate the target variable and save all resulting datasets and fitted transformers. Ensure the date index present in the input files is preserved in all output feature DataFrames.

**Input Files (located in the current directory):**
1.  `Cleaned_Train.csv` (training data)
2.  `Cleaned_Val.csv` (validation data)
3.  `Cleaned_Test.csv` (test data)
4.  `eda_result_summary.txt` (for insights, if helpful)
5.  `preliminary_analysis_report.txt` (for insights, if helpful)

**Target Variable:**
The target variable to be separated is named 'Close'.

**Date Index:**
The input CSV files have a date index (assume it's named 'numeric_date_idx' or infer from data if no specific name given in reports). This index MUST be preserved in all output feature DataFrames (`x_train.csv`, `x_val.csv`, `x_test.csv`).

**Core Tasks for `feature.py`:**

1.  **Load Data:**
    * Load `Cleaned_Train.csv`, `Cleaned_Val.csv`, and `Cleaned_Test.csv` into pandas DataFrames, ensuring the date index is correctly set.

2.  **Separate Target Variable:**
    * From each DataFrame (train, val, test), separate the target variable ('Close') into `y_train`, `y_val`, and `y_test`.
    * The remaining columns will form `X_train`, `X_val`, and `X_test`.

3.  **Missing Value Imputation (CRITICAL FOR CONSISTENCY):**
    * **Identify Column Types:** Automatically distinguish between numerical and categorical columns in `X_train`.
    * **Numerical Imputation:**
        * Initialize a `SimpleImputer` from `sklearn.impute` with a suitable strategy (e.g., 'median'). Use insights from the provided reports to choose the best strategy if possible.
        * **Fit this numerical imputer *ONLY* on the numerical columns of `X_train`.**
        * **Save the fitted numerical imputer object** to a file named `numerical_imputer.pkl` using `joblib`.
        * Use this *fitted* numerical imputer to **transform** the numerical columns in `X_train`, `X_val`, and `X_test`.
    * **Categorical Imputation:**
        * Initialize a `SimpleImputer` from `sklearn.impute` with a suitable strategy (e.g., 'most_frequent' or 'constant' with a fill_value like 'Missing'). Use insights from provided reports if possible.
        * **Fit this categorical imputer *ONLY* on the categorical columns of `X_train`.**
        * **Save the fitted categorical imputer object** to a file named `categorical_imputer.pkl` using `joblib`.
        * Use this *fitted* categorical imputer to **transform** the categorical columns in `X_train`, `X_val`, and `X_test`.
    * Ensure that after imputation, the DataFrames retain their original column names (for the imputed columns) and the preserved date index. Combine imputed numerical and categorical columns back into `X_train`, `X_val`, `X_test`.

4.  **Feature Scaling (Numerical Features - CRITICAL FOR CONSISTENCY):**
    * Apply this step *after* numerical imputation.
    * Initialize a `StandardScaler` from `sklearn.preprocessing`.
    * **Fit the `StandardScaler` *ONLY* on the numerical columns of the imputed `X_train`.**
    * **Save the fitted `StandardScaler` object** to a file named `scaler.pkl` using `joblib`.
    * Use this *fitted* scaler to **transform** the numerical columns in the imputed `X_train`, `X_val`, and `X_test`.
    * Ensure the output is a DataFrame with original numerical column names and the preserved date index.

5.  **Categorical Feature Encoding (CRITICAL FOR CONSISTENCY):**
    * Apply this step *after* categorical imputation.
    * Identify remaining categorical columns in the imputed `X_train`.
    * Initialize a `OneHotEncoder` from `sklearn.preprocessing` (set `handle_unknown='ignore'` and `sparse_output=False`).
    * **Fit the `OneHotEncoder` *ONLY* on the categorical columns of the imputed `X_train`.**
    * **Save the fitted `OneHotEncoder` object** (and any associated data like feature names if necessary for reconstruction) to a file named `one_hot_encoder.pkl` using `joblib`.
    * Use this *fitted* encoder to **transform** the categorical columns in `X_train`, `X_val`, and `X_test`.
    * Concatenate these newly encoded features with the numerical (and scaled) features. Ensure the date index is preserved. Be mindful of new column names generated by one-hot encoding.

6.  **Output Files:**
    * The script `feature.py` should save the following files to the current directory:
        * **Fitted Transformers:**
            * `numerical_imputer.pkl`
            * `categorical_imputer.pkl`
            * `scaler.pkl`
            * `one_hot_encoder.pkl`
        * **Processed Feature Sets (X):**
            * `x_train.csv` (containing fully processed features for training, with date index)
            * `x_val.csv` (containing fully processed features for validation, with date index)
            * `x_test.csv` (containing fully processed features for testing, with date index)
        * **Target Variables (y):**
            * `y_train.csv` (with date index and header)
            * `y_val.csv` (with date index and header)
            * `y_test.csv` (with date index and header)
    * The prompt from the user also mentioned saving `Features_Train.csv`, `Features_Val.csv`, `Features_Test.csv`. For clarity, assume these are equivalent to `x_train.csv`, `x_val.csv`, and `x_test.csv` respectively, unless explicitly defined otherwise by the insights from the provided reports. Prioritize saving `x_train.csv`, `x_val.csv`, `x_test.csv` along with their `y` counterparts.

**Important Considerations for Code Generation:**
* The script should use `pandas` for data manipulation and `sklearn` for transformations.
* Ensure all file operations (loading CSVs, saving CSVs, saving transformers with `joblib`) use paths relative to the current directory.
* The script should be robust and print informative messages about its progress (e.g., "Fitting numerical imputer on training data...", "Transforming test data...", "Saving x_train.csv...").
* Consider edge cases like no numerical columns, no categorical columns, or no NaNs found (the script should handle these gracefully).

This prompt gives GPT-4o a clear set of instructions, emphasizing the critical aspects of consistent application of transformers and saving them for later use (e.g., by the `ModelEvaluation_Agent`).
